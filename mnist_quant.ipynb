{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7bcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea625ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3925a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self,dict):\n",
    "        for key in dict:\n",
    "            setattr(self,key, dict[key]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dde357",
   "metadata": {},
   "source": [
    "### Training & Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25335943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch,max_iter = float('inf')):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > max_iter: break\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         print (data.shape)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f96a2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "#             print (data.shape)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43a099",
   "metadata": {},
   "source": [
    "### Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9ad670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiachuandeng/anaconda3/envs/latest_tf/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459065530/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "args = Args(dict( batch_size =64,\n",
    "                      test_batch_size= 1000,\n",
    "                        epochs = 2, \n",
    "                       lr =1.0,\n",
    "                    gamma=0.7,\n",
    "                    no_cuda = True,\n",
    "                    no_mps = True,\n",
    "                    dry_run=False,\n",
    "                     seed = 1,\n",
    "                    log_interval =10,\n",
    "                    save_model =True,\n",
    "                    model_dir=  './float_model/')\n",
    "               )\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,transform=transform)\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4498d",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5bfcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, quant=False):\n",
    "        # insert quant in conv2 and fc1\n",
    "        super(Model, self).__init__()\n",
    "        self.quant = quant\n",
    "        self.conv1 = nn.Conv2d(1,32,3,1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(9216, 128)\n",
    "        self.fc_relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        \n",
    "        if self.quant:\n",
    "            self.quant_conv = torch.quantization.QuantStub() \n",
    "\n",
    "            self.dequant_conv = torch.quantization.DeQuantStub()\n",
    "\n",
    "            self.quant_fc = torch.quantization.QuantStub() \n",
    "\n",
    "            self.dequant_fc = torch.quantization.DeQuantStub()\n",
    "    def forward(self,x):\n",
    "       \n",
    "        x = self.conv1 (x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # insert quant in conv2\n",
    "        if  self.quant:\n",
    "            x = self.quant_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu2(x)\n",
    "        if self.quant:\n",
    "            x = self.dequant_conv(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x,1)\n",
    "       \n",
    "        # insert quant in fc\n",
    "        if self.quant:\n",
    "            x = self.quant_fc(x)\n",
    "        x  = self.fc(x)\n",
    "        x = self.fc_relu(x)\n",
    "        if self.quant:\n",
    "            x = self.dequant_fc(x)\n",
    "        \n",
    "        \n",
    "        x = self.fc2 (x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d56ca",
   "metadata": {},
   "source": [
    "### Train Float Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0047b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_main():\n",
    "    # Training settings\n",
    "    \n",
    "     \n",
    "    \n",
    "    if not os.path.exists(args.model_dir):\n",
    "         \n",
    "        os.system('mkdir -p {}'.format(args.model_dir))\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "\n",
    "    model =  Model(quant=False).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), os.path.join(args.model_dir,\"mnist_cnn.pt\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b80a0ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321632\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.557610\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.028888\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.490685\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.311797\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.221344\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.193964\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.357805\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.287615\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.155159\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.132312\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.251371\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.191137\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.184157\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.185434\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.144271\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.178386\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.095577\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.389366\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.140725\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.103709\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.123493\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.051220\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.326020\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.116323\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.318939\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.192092\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.061062\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.141457\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.065876\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.172922\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.168607\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.008845\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.155847\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.004022\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.019391\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.068447\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.156767\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.011885\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.044096\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.010984\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.015569\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.276532\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.220094\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.057721\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.028655\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.058902\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.052499\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.031329\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.070741\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.063207\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.057972\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.110284\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.018201\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.030363\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.169285\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.135749\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.030580\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.038254\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.102628\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.170413\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.024858\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.049542\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.080030\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.092314\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.037037\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.021946\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.059523\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.151069\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.005867\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.085900\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.099215\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.200407\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.172428\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.064745\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.049950\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.013162\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.016112\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.038204\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.068264\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.099222\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.007675\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.022907\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.258356\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.096391\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.012212\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.037383\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.135972\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.014631\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.099349\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.075497\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.005211\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.006060\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.001476\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9839/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.040630\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.016353\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.030145\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.096567\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.030065\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.038410\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.008687\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.033342\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.098838\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.034113\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.184511\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.087003\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.052975\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.043135\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.043252\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.013520\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.068333\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.003247\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.046223\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.016461\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.082994\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.010358\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.017137\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.056623\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.031128\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.012309\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.087893\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.000778\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.004089\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.019264\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.025088\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.039287\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.005747\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.099025\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.017209\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.003474\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.028018\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.061023\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.003251\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.003627\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.053001\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.002567\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.113033\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.105937\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.048838\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.003180\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.007824\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.022821\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.003299\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.003739\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.077800\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.024124\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.062225\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.001467\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.003918\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.205901\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.102447\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.001266\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.027917\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.042212\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.098876\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.002129\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.013535\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.008577\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.090719\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.013791\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.011654\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.006891\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.062268\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.006615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.071646\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.022364\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.120342\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.100447\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.037646\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.020916\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.011118\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.004200\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.010997\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.022287\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.135189\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.006764\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.006627\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.019448\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.088374\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.009088\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.000792\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.079718\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.010020\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.019572\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.018429\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.000537\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.000637\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.000139\n",
      "\n",
      "Test set: Average loss: 0.0369, Accuracy: 9880/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f34ff8",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b61ccd",
   "metadata": {},
   "source": [
    "#### init quant model with float model's params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13fb0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def copy_float2quant(m_float,m_quant):\n",
    "    m_quant_dict = m_quant.state_dict()\n",
    "    for name , param in m_float.state_dict().items():\n",
    "        m_quant_dict[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8791f89",
   "metadata": {},
   "source": [
    "#### Post  training quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04560867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_quant(m_float,m_quant):\n",
    "    copy_float2quant(m_float,m_quant)\n",
    " \n",
    "    torch.quantization.fuse_modules(m_quant, ['conv1','relu1'], inplace=True)\n",
    "\n",
    "    torch.quantization.fuse_modules(m_quant, ['fc','fc_relu'], inplace=True)\n",
    " \n",
    " \n",
    "\n",
    "    # \"\"\"Prepare\"\"\"\n",
    "    qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "    for module_name, module in m_quant.named_children():\n",
    "#         print (module_name)\n",
    "        if module_name not in ['conv1','relu1','fc2']:\n",
    "       \n",
    "            module.qconfig = qconfig\n",
    "\n",
    "    torch.quantization.prepare(m_quant, inplace=True)\n",
    "\n",
    "    # \"\"\"Calibrate\n",
    "    # - This example uses random data for convenience. Use representative (validation) data instead.\n",
    "    # \"\"\"\n",
    "    print ('calibrating...')\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if batch_idx  > 100: break\n",
    "            m_quant(data)\n",
    "\n",
    "\n",
    "    # \"\"\"Convert\"\"\"\n",
    "    torch.quantization.convert(m_quant, inplace=True,remove_qconfig = False)\n",
    "#     print (m_quant)\n",
    "    print ('m_quant on test data:')\n",
    "    test(m_quant, torch.device(\"cpu\"), test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42de628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_float on test data:\n",
      "\n",
      "Test set: Average loss: 0.0369, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "calibrating...\n",
      "m_quant on test data:\n",
      "\n",
      "Test set: Average loss: 0.0369, Accuracy: 9878/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_float = Model(quant=False)\n",
    "m_float.load_state_dict(torch.load(os.path.join(args.model_dir,'mnist_cnn.pt'),map_location='cpu'))\n",
    "m_float.eval()\n",
    "print ('m_float on test data:')\n",
    "test(m_float, torch.device(\"cpu\"), test_loader)\n",
    "\n",
    "m_quant = Model(quant=True)\n",
    "m_quant.eval()\n",
    "post_training_quant(m_float,m_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484ec6a",
   "metadata": {},
   "source": [
    "#### QAT\n",
    "points to note: https://pytorch.org/blog/quantization-in-practice/\n",
    "![Screen Shot 2023-02-23 at 18 36 20](https://user-images.githubusercontent.com/20760190/220883386-1e85bf80-284c-4fac-8933-2323fc7e12f1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ee173c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qat_main(model):\n",
    "    # Training settings\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(args.model_dir):\n",
    "\n",
    "        os.system('mkdir -p {}'.format(args.model_dir))\n",
    "\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr*0.01) # qat typicall requires using 1% of original learning rate\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, 2):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), os.path.join(args.model_dir,\"mnist_cnn_qat.pt\"))\n",
    "\n",
    "\n",
    "def QAT(m_float,m_quant,init_with_float = True):\n",
    "    \n",
    "    if init_with_float:\n",
    "        print (\"init with pre-trained float model\")\n",
    "        copy_float2quant(m_float,m_quant)\n",
    "    else:\n",
    "        print ('train qat from scratch...')\n",
    "\n",
    "#    fuse model\n",
    "    torch.quantization.fuse_modules(m_quant, ['conv1','relu1'], inplace=True)\n",
    "#     torch.quantization.fuse_modules(m_quant, ['conv2','bn','relu2'], inplace=True)\n",
    "\n",
    "    torch.quantization.fuse_modules(m_quant, ['fc','fc_relu'], inplace=True)\n",
    "\n",
    " \n",
    "\n",
    "    # \"\"\"Prepare\"\"\"\n",
    "    qconfig  = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "    for module_name, module in m_quant.named_children():\n",
    "        print (module_name)\n",
    "        if module_name not in ['conv1','relu1','fc2']:\n",
    "            print (module,'set config')\n",
    "            module.qconfig = qconfig\n",
    "\n",
    "    torch.quantization.prepare_qat(m_quant, inplace=True)\n",
    "    \n",
    "    qat_main(m_quant)\n",
    "    \n",
    "\n",
    "#     # \"\"\"Convert\"\"\"\n",
    "    torch.quantization.convert(m_quant, inplace=True,remove_qconfig = False)\n",
    "    \n",
    "    print ('QAT m_quant on test data:')\n",
    "    test(m_quant, torch.device(\"cpu\"), test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c1b8",
   "metadata": {},
   "source": [
    "#### QAT training\n",
    "with pre-trained float model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a02700df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_float on test data:\n",
      "\n",
      "Test set: Average loss: 0.0369, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "init with pre-trained float model\n",
      "conv1\n",
      "relu1\n",
      "conv2\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) set config\n",
      "bn\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) set config\n",
      "relu2\n",
      "ReLU() set config\n",
      "fc\n",
      "LinearReLU(\n",
      "  (0): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ") set config\n",
      "fc_relu\n",
      "Identity() set config\n",
      "fc2\n",
      "quant_conv\n",
      "QuantStub() set config\n",
      "dequant_conv\n",
      "DeQuantStub() set config\n",
      "quant_fc\n",
      "QuantStub() set config\n",
      "dequant_fc\n",
      "DeQuantStub() set config\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.065287\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.011470\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.030812\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.037720\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.028030\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.042262\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.000449\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.017825\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.046835\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.028288\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.125774\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.138343\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.007914\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.021567\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.059149\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.031913\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.197453\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.004054\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.008901\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.067627\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.052514\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.001865\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.003783\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.024524\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.007841\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.017371\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.074713\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.001112\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.002371\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.044722\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.006792\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.017355\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.001183\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.057659\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.001683\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.005457\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.028439\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.007420\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.001805\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.001422\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.013472\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.002611\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.033542\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.099906\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.012102\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.000919\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.028587\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.019287\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.029741\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.022712\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.012227\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.002357\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.008168\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.001377\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.005200\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.110486\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.009486\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.005181\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.010917\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.021429\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.063681\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.000580\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.038184\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.000799\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.012471\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.004608\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.002962\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.006285\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.029563\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.009691\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.008768\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.010137\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.020375\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.051450\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.038143\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.003591\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.009699\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.013729\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.003614\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.008842\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.061491\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.005179\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.007408\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.013161\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.030267\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.001379\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.000782\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.008058\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.005854\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.002255\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.011111\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.000463\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.000308\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.000167\n",
      "\n",
      "Test set: Average loss: 0.0305, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "QAT m_quant on test data:\n",
      "\n",
      "Test set: Average loss: 0.0304, Accuracy: 9904/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_float = Model(quant=False)\n",
    "m_float.load_state_dict(torch.load(os.path.join(args.model_dir,'mnist_cnn.pt'),map_location='cpu'))\n",
    "m_float.eval()\n",
    "print ('m_float on test data:')\n",
    "test(m_float, torch.device(\"cpu\"), test_loader)\n",
    "\n",
    "m_quant = Model(quant=True)\n",
    "m_quant.eval()\n",
    "\n",
    "QAT(m_float,m_quant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f5354",
   "metadata": {},
   "source": [
    "without float model initialization, train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ea6cc1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train qat from scratch...\n",
      "conv1\n",
      "relu1\n",
      "conv2\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) set config\n",
      "bn\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) set config\n",
      "relu2\n",
      "ReLU() set config\n",
      "fc\n",
      "LinearReLU(\n",
      "  (0): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ") set config\n",
      "fc_relu\n",
      "Identity() set config\n",
      "fc2\n",
      "quant_conv\n",
      "QuantStub() set config\n",
      "dequant_conv\n",
      "DeQuantStub() set config\n",
      "quant_fc\n",
      "QuantStub() set config\n",
      "dequant_fc\n",
      "DeQuantStub() set config\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.347924\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.895264\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.639413\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.293633\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.995529\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.895542\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.693052\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.669555\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.794013\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.589892\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.566987\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.479809\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.533355\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.418727\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.385428\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.393265\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.519274\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.356939\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.585242\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.392631\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.299635\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.250102\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.468109\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.590463\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.271710\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.512956\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.399078\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.201014\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.254583\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.363722\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.362333\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.246353\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.188789\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.310909\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.106753\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.257204\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.362657\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.400732\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.143530\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.262956\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.170982\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.242157\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.340031\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.211934\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.253994\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.171911\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.191340\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.277710\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.225536\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.286917\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.257835\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.276626\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.207833\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.081575\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.164893\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.243887\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.184686\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.126796\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.271668\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.199566\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.120116\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.090610\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.150925\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.145773\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.255788\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.154862\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.107474\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.277273\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.230878\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.123182\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.238314\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.360055\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.253095\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.229824\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.189052\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.113824\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.145849\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.097438\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.118793\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.217673\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.201415\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.116842\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.048689\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.202272\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.086055\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.084623\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.113268\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.183781\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.091948\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.090617\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.189626\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.066360\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.018283\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.025230\n",
      "\n",
      "Test set: Average loss: 0.1305, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "QAT m_quant on test data:\n",
      "\n",
      "Test set: Average loss: 0.1307, Accuracy: 9645/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_quant = Model(quant=True)\n",
    "m_quant.eval()\n",
    "\n",
    "QAT(None,m_quant,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd510cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
